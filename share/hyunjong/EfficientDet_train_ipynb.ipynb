{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "O_tnPGK74wUM",
        "outputId": "c6e7e511-cd30-4b08-ec0f-76be2eeb609d"
      },
      "outputs": [],
      "source": [
        "#!pip install effdet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n5QtMnjy4g2y"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 및 모듈 import\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xu7QfZ-14g28"
      },
      "outputs": [],
      "source": [
        "# CustomDataset class 선언\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    '''\n",
        "      data_dir: data가 존재하는 폴더 경로\n",
        "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, annotation, data_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        \n",
        "        # coco annotation 불러오기 (by. coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.predictions = {\n",
        "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
        "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
        "            \"annotations\": None\n",
        "        }\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "        \n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # boxes (x, y, w, h)\n",
        "        boxes = np.array([x['bbox'] for x in anns])\n",
        "\n",
        "        # boxex (x_min, y_min, x_max, y_max)\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "        \n",
        "        # box별 label\n",
        "        labels = np.array([x['category_id'] for x in anns])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        \n",
        "        areas = np.array([x['area'] for x in anns])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        \n",
        "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
        "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
        "\n",
        "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
        "                  'iscrowd': is_crowds}\n",
        "\n",
        "        # transform\n",
        "        if self.transforms:\n",
        "            while True:\n",
        "                sample = self.transforms(**{\n",
        "                    'image': image,\n",
        "                    'bboxes': target['boxes'],\n",
        "                    'labels': labels\n",
        "                })\n",
        "                if len(sample['bboxes']) > 0:\n",
        "                    image = sample['image']\n",
        "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
        "                    target['labels'] = torch.tensor(sample['labels'])\n",
        "                    break\n",
        "            \n",
        "        return image, target, image_id\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k_mYQSta4g2_"
      },
      "outputs": [],
      "source": [
        "# Albumentation을 이용, augmentation 선언\n",
        "p = 0.5\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(1024, 1024),\n",
        "        A.Cutout(always_apply=False, p=p, num_holes=8, max_h_size=8, max_w_size=8),\n",
        "        A.HorizontalFlip(always_apply=False, p=p),\n",
        "        A.VerticalFlip(always_apply=False, p=p),\n",
        "        #A.ToGray(always_apply=False, p=p), // 색상 정보가 중요하면 disable\n",
        "        A.GaussNoise(always_apply=False, p=p, var_limit=(400.0, 500.0)),\n",
        "        A.RandomRotate90(always_apply=False, p=p),\n",
        "        A.MotionBlur(always_apply=False, p=p, blur_limit=(10, 20)),#촬영 중 움직이는 경우, EDA에서 존재 확인\n",
        "        #A.Blur(always_apply=False, p=p, blur_limit=(3, 20)),\n",
        "        #A.CLAHE(always_apply=False, p=p, clip_limit=(1, 4), tile_grid_size=(8, 8))#뿌연 이미지 경우\n",
        "        A.RandomBrightnessContrast(always_apply=False, p=p, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
        "        A.HueSaturationValue(always_apply=False, p=p, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),  \n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(1024, 1024),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lx1ih23h4g3C"
      },
      "outputs": [],
      "source": [
        "# loss 추적\n",
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ERF2BAfo4g3D"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# Effdet config\n",
        "# https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/config/model_config.py\n",
        "\n",
        "# Effdet config를 통해 모델 불러오기\n",
        "def get_net(checkpoint_path=None):\n",
        "    \n",
        "    config = get_efficientdet_config('tf_efficientdet_d3')\n",
        "    config.num_classes = 10\n",
        "    config.image_size = (1024,1024)\n",
        "    \n",
        "    config.soft_nms = False\n",
        "    config.max_det_per_image = 25\n",
        "    \n",
        "    net = EfficientDet(config, pretrained_backbone=True)\n",
        "    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        net.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "    return DetBenchTrain(net)\n",
        "    \n",
        "# train function\n",
        "def train_fn(num_epochs, train_data_loader, optimizer, model, device, clip=35):\n",
        "    loss_hist = Averager()\n",
        "    model.train()\n",
        "\n",
        "    #loss = None\n",
        "    wandb.init(project='mmdetction', name='EfficientDet base code')\n",
        "    wandb.watch(model, log='all', log_freq=10)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_hist.reset()\n",
        "        \n",
        "        example_images = []\n",
        "        for images, targets, image_ids in tqdm(train_data_loader):\n",
        "            \n",
        "                images = torch.stack(images) # bs, ch, w, h - 16, 3, 512, 512\n",
        "                images = images.to(device).float()\n",
        "                boxes = [target['boxes'].to(device).float() for target in targets]\n",
        "                labels = [target['labels'].to(device).float() for target in targets]\n",
        "                target = {\"bbox\": boxes, \"cls\": labels}\n",
        "\n",
        "                # calculate loss&\n",
        "                loss, cls_loss, box_loss = model(images, target).values()\n",
        "                \n",
        "                loss_value = loss.detach().item()\n",
        "                \n",
        "                loss_hist.send(loss_value)\n",
        "                \n",
        "                # backward\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # grad clip\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                \n",
        "                optimizer.step()\n",
        "\n",
        "                # add wandb image\n",
        "                #example_images.append( wandb.Image(images[0], caption='Pred : {} Truth : {}'.format()     ) ) )\n",
        "        \n",
        "        wandb.log({'Epoch':epoch+1, 'loss':loss_hist.value})\n",
        "\n",
        "        print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\")\n",
        "        torch.save(model.state_dict(), f'epoch_{epoch+1}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0mbJK-ae4g3E"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    annotation = '../dataset/train.json'\n",
        "    #annotation = '../dataset/stratified_train.10fold.wArea.json'\n",
        "    data_dir = '../dataset'\n",
        "    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n",
        "\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(device)\n",
        "\n",
        "    model = get_net()\n",
        "    model.to(device)\n",
        "    \n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    #optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "    optimizer = torch.optim.AdamW(params, lr=0.005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    num_epochs = 12\n",
        "\n",
        "    loss = train_fn(num_epochs, train_data_loader, optimizer, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UuCiT5tZ4g3G",
        "outputId": "cb631ed3-95a0-4d9b-e0ac-c2a6e45c33a5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/detection/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:690: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.18s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfonawork\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/fonawork/mmdetction/runs/45lpk1rr\" target=\"_blank\">EfficientDet base code</a></strong> to <a href=\"https://wandb.ai/fonawork/mmdetction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 1200/1221 [17:47<00:18,  1.12it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31235/2834939114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_31235/2707852726.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_31235/666304950.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(num_epochs, train_data_loader, optimizer, model, device, clip)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# grad clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/detection/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/detection/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "seed = 123456\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EfficientDet_train.ipynb의 사본",
      "provenance": []
    },
    "interpreter": {
      "hash": "b94c6de4bce9a87a354a5fa9998691adc0532adddb9d4140f5ba941d00b01fae"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('detection': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
